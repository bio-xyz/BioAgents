### elizaOS Environment Variables ###
# To get started, copy this file to .env, or make a .env and add the settings you'd like to override
# Please read the comments for each of the configurations

# The only thing you ABSOLUTELY NEED to get up and running is one of the model provider keys, 
# i.e. OPENAI_API_KEY or ANTHROPIC_API_KEY, or setup the local-ai plugin
# Everything else is optional, and most settings and secrets can be configured in your agent or through the GUI
# For multi-agent, each agent will need keys for the various services it is connected to
# You can use the .env or environment variables generally for shared keys, such as to model providers, 
# database, etc, with scoped keys for services such as Telegram, Discord, etc

### MODEL PROVIDER KEYS ###
# Eliza is compatible with a wide array of model providers. Many have OpenAI compatible APIs, 
# and you can use them by overriding the base URL

# NOTE: You will need a provider that provides embeddings. So even if you use Claude, you will 
# need to get embeddings using another provider, for example openai or our local-ai plugin

# OpenAI Configuration (for embeddings)
OPENAI_API_KEY=
# Use this to override the openai endpoint, for example for using together.ai, fireworks or other providers
# OPENAI_BASE_URL=

# Anthropic Configuration (if you wanna use Anthropic)
ANTHROPIC_API_KEY=

### LOCAL AI CONFIGURATION ### (Local AI plugins)
USE_LOCAL_AI=
USE_STUDIOLM_TEXT_MODELS=
USE_OLLAMA_TEXT_MODELS=

# Ollama Configuration (if you're using Ollama)
OLLAMA_API_ENDPOINT=
OLLAMA_MODEL=
USE_OLLAMA_EMBEDDING=
OLLAMA_EMBEDDING_MODEL=
OLLAMA_SMALL_MODEL=
OLLAMA_MEDIUM_MODEL=
OLLAMA_LARGE_MODEL=

# StudioLM Configuration (if you're using StudioLM)
STUDIOLM_SERVER_URL=
STUDIOLM_SMALL_MODEL=
STUDIOLM_MEDIUM_MODEL=
STUDIOLM_EMBEDDING_MODEL=

### DATABASE ###
# By default, Eliza will use a local pglite instance
# If you fill out POSTGRES_URL, the agent will connect to your postgres instance instead of using the local path

# You can override the pglite data directory
# PGLITE_DATA_DIR=/Users/UserName/eliza/packages/.pglite/

# Fill this out if you want to use Postgres
POSTGRES_URL=

### LOGGING CONFIGURATION ###
# Logging Configuration (supported: fatal, error, warn, info, debug, trace | default: info)
LOG_LEVEL=

### SINGLE AGENT VARIABLES ###
# If you are running multiple agents, you will need to configure these variables in the agent secrets 
# (available in the GUI) OR you can namespace the secrets and connect them up in your character definition

# Example: 
# settings: {
#   process.env.COMMUNITY_MANAGER_DISCORD_API_TOKEN
# }

# Note: See below for multi-agent examples

# Discord Configuration
DISCORD_APPLICATION_ID=
DISCORD_API_TOKEN=

# Telegram Configuration
TELEGRAM_BOT_TOKEN=

TWITTER_API_KEY=
TWITTER_API_SECRET_KEY=
TWITTER_ACCESS_TOKEN=
TWITTER_ACCESS_TOKEN_SECRET=

# Basic Configuration
TWITTER_DRY_RUN=true              # Test mode without posting
TWITTER_RETRY_LIMIT=5              # Max retry attempts
TWITTER_ENABLE_DISCOVERY=false
TWITTER_ENABLE_REPLIES=true

# Post Generation
TWITTER_ENABLE_POST=true
TWITTER_POST_INTERVAL_MIN=45       # Min interval (minutes)
TWITTER_POST_INTERVAL_MAX=90     # Max interval (minutes)
TWITTER_POST_IMMEDIATELY=false     # Post on startup
TWITTER_POST_INTERVAL_VARIANCE=0.2 # Interval variance (0.0-1.0)

# Engagement Settings
TWITTER_MAX_ENGAGEMENTS_PER_RUN=10    # Max interactions per cycle
TWITTER_ENGAGEMENT_INTERVAL=1     # Fixed interval for interactions (default: 30, used if MIN/MAX not set)
TWITTER_ENGAGEMENT_INTERVAL_MIN=1 # Minimum minutes between engagements (default: 20)
TWITTER_ENGAGEMENT_INTERVAL_MAX=2 

# Load knowledge plugin documents on startup
LOAD_DOCS_ON_STARTUP=true

# Knowledge graph setup settings
KG_TRIPLE_STORE_URL=
GCP_JSON_CREDENTIALS=
KG_GOOGLE_DRIVE_FOLDER_ID=

# Our LLM settings
OPENROUTER_API_KEY=
EMBEDDING_PROVIDER=openai
TEXT_EMBEDDING_MODEL=text-embedding-3-large
TEXT_PROVIDER=openrouter
TEXT_MODEL=anthropic/claude-opus-4
OPENROUTER_LARGE_MODEL=anthropic/claude-opus-4
OPENROUTER_SMALL_MODEL=google/gemini-2.5-flash
KG_GENERATION_MODEL=gpt-4.1

# Knowledge plugin loading config
MAX_CONCURRENT_REQUESTS=1
REQUESTS_PER_MINUTE=60
TOKENS_PER_MINUTE=150000
MAX_INPUT_TOKENS=2000
MAX_OUTPUT_TOKENS=2000

MAX_DOCUMENTS_TO_PROCESS=1
DOCUMENT_PROCESSING_BATCH_SIZE=1
CHUNK_SIZE=2000
CHUNK_OVERLAP=50
MAX_CHUNKS_PER_DOCUMENT=5
CHUNK_PROCESSING_DELAY_MS=2000

CTX_KNOWLEDGE_ENABLED=true
KNOWLEDGE_CHUNKS_PER_ANSWER=10

# Langfuse config
LANGFUSE_SECRET_KEY=
LANGFUSE_PUBLIC_KEY=
LANGFUSE_BASEURL=

# Evaluation script config
EVALUATE_OTHER_MODEL=
EVALUATION_EXTERNAL_MODEL=
MODEL_TO_EVALUATE_WITH=openai/gpt-4.1

# Remote agent config URL (e.g. )
AGENT_CONFIG_URL='https://gist.githubusercontent.com/mygithub/58391035180310931/raw'

# Privy (for auth)
PRIVY_APP_ID=
PRIVY_APP_SECRET=

# Perplexity (for Twitter posts)
PERPLEXITY_API_KEY=

# Environment ("development", "test", "production")
NODE_ENV="development

# Server to server api key (add your own safe key, can be anything)
ELIZA_SERVER_AUTH_TOKEN=x-api..