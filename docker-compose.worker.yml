# ==========================================================================
# Worker-Only Docker Compose
# ==========================================================================
#
# Deploy this to ANY server to add more worker capacity.
# All workers connect to the same Redis queue and share the load.
#
# SETUP:
#   1. Copy .env.worker.example to .env
#   2. Fill in your credentials
#   3. Run: docker-compose -f docker-compose.worker.yml up -d
#
# SCALE:
#   docker-compose -f docker-compose.worker.yml up -d --scale worker=3
#
# LOGS:
#   docker-compose -f docker-compose.worker.yml logs -f
#
# ==========================================================================
services:
  worker:
    build: .
    command: ["bun", "run", "src/worker.ts"]

    # Coolify label to override default 30s stop timeout
    labels:
      - coolify.stopTimeout=28800

    environment:
      # Required: Redis URL (Upstash or your managed Redis)
      - REDIS_URL=${REDIS_URL}
      - USE_JOB_QUEUE=true

      # Required: Database
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_ANON_KEY=${SUPABASE_ANON_KEY}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY:-}
      - SUPABASE_FULL_URL=${SUPABASE_FULL_URL:-}

      # Authentication
      - BIOAGENTS_SECRET=${BIOAGENTS_SECRET:-}
      - AUTH_MODE=${AUTH_MODE:-none}

      # Required: LLM API Keys
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - COHERE_API_KEY=${COHERE_API_KEY:-}

      # Agent Configuration
      - OPENSCHOLAR_API_URL=${OPENSCHOLAR_API_URL:-}
      - REPLY_LLM_PROVIDER=${REPLY_LLM_PROVIDER:-google}
      - REPLY_LLM_MODEL=${REPLY_LLM_MODEL:-gemini-2.5-pro}
      - HYP_LLM_PROVIDER=${HYP_LLM_PROVIDER:-google}
      - HYP_LLM_MODEL=${HYP_LLM_MODEL:-gemini-2.5-pro}
      - PLANNING_LLM_PROVIDER=${PLANNING_LLM_PROVIDER:-google}
      - PLANNING_LLM_MODEL=${PLANNING_LLM_MODEL:-gemini-2.5-flash}
      - STRUCTURED_LLM_PROVIDER=${STRUCTURED_LLM_PROVIDER:-google}
      - STRUCTURED_LLM_MODEL=${STRUCTURED_LLM_MODEL:-gemini-2.5-flash}

      # Continue Research Agent
      - CONTINUE_RESEARCH_LLM_PROVIDER=${CONTINUE_RESEARCH_LLM_PROVIDER:-anthropic}
      - CONTINUE_RESEARCH_LLM_MODEL=${CONTINUE_RESEARCH_LLM_MODEL:-claude-sonnet-4-5-20250929}
      - MAX_AUTO_ITERATIONS=${MAX_AUTO_ITERATIONS:-5}

      # Embedding Configuration
      - EMBEDDING_PROVIDER=${EMBEDDING_PROVIDER:-openai}
      - TEXT_EMBEDDING_MODEL=${TEXT_EMBEDDING_MODEL:-text-embedding-3-large}

      # Literature & Analysis Agents
      - PRIMARY_LITERATURE_AGENT=${PRIMARY_LITERATURE_AGENT:-}
      - PRIMARY_ANALYSIS_AGENT=${PRIMARY_ANALYSIS_AGENT:-}
      - EDISON_API_URL=${EDISON_API_URL:-}
      - EDISON_API_KEY=${EDISON_API_KEY:-}
      - BIOAGENTS_API_URL=${BIOAGENTS_API_URL:-}
      - BIO_LIT_AGENT_API_URL=${BIO_LIT_AGENT_API_URL:-}
      - BIO_LIT_AGENT_API_KEY=${BIO_LIT_AGENT_API_KEY:-}
      - DATA_ANALYSIS_API_URL=${DATA_ANALYSIS_API_URL:-}
      - DATA_ANALYSIS_API_KEY=${DATA_ANALYSIS_API_KEY:-}

      # Task Timeouts
      - BIO_ANALYSIS_TASK_TIMEOUT_MINUTES=${BIO_ANALYSIS_TASK_TIMEOUT_MINUTES:-60}
      - BIO_LITERATURE_TASK_TIMEOUT_MINUTES=${BIO_LITERATURE_TASK_TIMEOUT_MINUTES:-60}
      - EDISON_TASK_TIMEOUT_MINUTES=${EDISON_TASK_TIMEOUT_MINUTES:-30}
      - FILE_STATUS_TTL_MINUTES=${FILE_STATUS_TTL_MINUTES:-60}

      # Vector Search Configuration
      - CHUNK_SIZE=${CHUNK_SIZE:-2000}
      - CHUNK_OVERLAP=${CHUNK_OVERLAP:-200}
      - VECTOR_SEARCH_LIMIT=${VECTOR_SEARCH_LIMIT:-20}
      - RERANK_FINAL_LIMIT=${RERANK_FINAL_LIMIT:-5}
      - USE_RERANKING=${USE_RERANKING:-true}
      - SIMILARITY_THRESHOLD=${SIMILARITY_THRESHOLD:-0.45}
      - RERANKER_SCORE_THRESHOLD=${RERANKER_SCORE_THRESHOLD:-0.3}
      - KNOWLEDGE_DOCS_PATH=${KNOWLEDGE_DOCS_PATH:-docs}

      # Character Configuration
      - CHARACTER_FILE=${CHARACTER_FILE:-characters/bios.json}
      - CHARACTER_JSON=${CHARACTER_JSON:-}
      - AGENT_NAME=${AGENT_NAME:-}
      - AGENT_EMAIL=${AGENT_EMAIL:-}

      # Worker Concurrency
      - CHAT_QUEUE_CONCURRENCY=${CHAT_QUEUE_CONCURRENCY:-5}
      - DEEP_RESEARCH_QUEUE_CONCURRENCY=${DEEP_RESEARCH_QUEUE_CONCURRENCY:-3}
      - PAPER_GENERATION_CONCURRENCY=${PAPER_GENERATION_CONCURRENCY:-1}
      - MAX_CONCURRENT_PAPER_JOBS=${MAX_CONCURRENT_PAPER_JOBS:-3}

      # Storage Configuration
      - STORAGE_PROVIDER=${STORAGE_PROVIDER:-}
      - S3_ENDPOINT=${S3_ENDPOINT:-}
      - S3_BUCKET=${S3_BUCKET:-}
      - S3_REGION=${S3_REGION:-}
      - S3_ACCESS_KEY=${S3_ACCESS_KEY:-}
      - S3_SECRET_KEY=${S3_SECRET_KEY:-}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-}
      - AWS_REGION=${AWS_REGION:-}

      # Production
      - NODE_ENV=production

    restart: unless-stopped

    # CRITICAL: Allow workers to finish long-running jobs before stopping
    # Default is 10s, but deep research jobs can run up to 8 hours
    # Docker will wait this long for graceful shutdown before force killing
    stop_grace_period: 8h

    # Health check - verify worker is running
    healthcheck:
      test: ["CMD", "pgrep", "-f", "worker"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

    # Resource limits (adjust based on your server)
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
